{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14136459,"sourceType":"datasetVersion","datasetId":9008283},{"sourceId":13484043,"sourceType":"datasetVersion","datasetId":8560708}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==============================================================================\n# CELL 1: Load Trained Model from Checkpoint\n# ==============================================================================\n\nimport os\nimport sys\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport wfdb\n\n# Add ESI to path if needed\nesi_path = \"/kaggle/input/esi-repo/ESI\"  # Adjust based on your ESI input dataset\nif os.path.exists(esi_path) and esi_path not in sys.path:\n    sys.path.append(esi_path)\n\nfrom model.convnextv2 import convnextv2_base\n\n# Set device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"üñ•Ô∏è Using device: {device}\")\n\n# ==============================================================================\n# Model Architecture (MUST match training)\n# ==============================================================================\n\nclass MultiScaleTemporalHead(nn.Module):\n    def __init__(self, input_dim=1024, hidden_dim=256, lstm_layers=2, dropout=0.3):\n        super().__init__()\n        \n        self.lstm = nn.LSTM(\n            input_dim, hidden_dim,\n            num_layers=2,\n            batch_first=True,\n            bidirectional=True,\n            dropout=dropout if lstm_layers > 1 else 0\n        )\n        \n        self.attention = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, 1),\n            nn.Softmax(dim=1)\n        )\n        \n        self.global_pool = nn.AdaptiveAvgPool1d(1)\n        self.max_pool = nn.AdaptiveMaxPool1d(1)\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_dim * 2 + input_dim * 2, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, 64),\n            nn.LayerNorm(64),\n            nn.ReLU(),\n            nn.Dropout(dropout * 0.5),\n            nn.Linear(64, 1)\n        )\n        \n        self._init_weights()\n    \n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight, gain=0.5)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.LSTM):\n                for name, param in m.named_parameters():\n                    if 'weight' in name:\n                        nn.init.orthogonal_(param)\n                    elif 'bias' in name:\n                        nn.init.constant_(param, 0)\n    \n    def forward(self, x):\n        x = x + 1e-8\n        x_seq = x.unsqueeze(1)\n        lstm_out, _ = self.lstm(x_seq)\n        attn_weights = self.attention(lstm_out)\n        attended = (lstm_out * attn_weights).sum(dim=1)\n        \n        x_unsqueezed = x.unsqueeze(2)\n        avg_pooled = self.global_pool(x_unsqueezed).squeeze(2)\n        max_pooled = self.max_pool(x_unsqueezed).squeeze(2)\n        \n        combined = torch.cat([attended, avg_pooled, max_pooled], dim=1)\n        combined = torch.clamp(combined, -10, 10)\n        logits = self.classifier(combined)\n        \n        return logits\n\n\nclass ESIApneaDetector(nn.Module):\n    def __init__(self, checkpoint_path, config):\n        super().__init__()\n        \n        self.backbone = convnextv2_base(\n            in_chans=12,\n            num_classes=5,\n            return_embedding=True\n        )\n        \n        if os.path.exists(checkpoint_path):\n            try:\n                from safetensors.torch import load_file\n                state_dict = load_file(checkpoint_path, device=\"cpu\")\n                clean_weights = {\n                    k.replace(\"img_encoder.\", \"\"): v\n                    for k, v in state_dict.items()\n                    if \"img_encoder.\" in k\n                }\n                self.backbone.load_state_dict(clean_weights, strict=False)\n                print(\"‚úÖ ESI weights loaded\")\n            except Exception as e:\n                print(f\"‚ö†Ô∏è Could not load ESI weights: {e}\")\n        \n        self.lead_projection = nn.Sequential(\n            nn.Conv1d(1, 12, kernel_size=1),\n            nn.BatchNorm1d(12)\n        )\n        \n        nn.init.xavier_uniform_(self.lead_projection[0].weight, gain=0.5)\n        \n        for param in self.backbone.parameters():\n            param.requires_grad = False\n        \n        self.head = MultiScaleTemporalHead(\n            input_dim=config.ESI_EMBEDDING_DIM,\n            hidden_dim=config.HIDDEN_DIM,\n            lstm_layers=config.LSTM_LAYERS,\n            dropout=config.DROPOUT\n        )\n    \n    def forward(self, x):\n        if torch.isnan(x).any():\n            x = torch.nan_to_num(x, nan=0.0)\n        \n        x_multi = self.lead_projection(x)\n        x_multi = torch.clamp(x_multi, -10, 10)\n        x_multi = x_multi.permute(0, 2, 1)\n        features = self.backbone(x_multi)\n        \n        if torch.isnan(features).any():\n            features = torch.nan_to_num(features, nan=0.0)\n        \n        logits = self.head(features)\n        return logits\n\n\n# ==============================================================================\n# Load Model Function\n# ==============================================================================\n\ndef load_trained_model(checkpoint_path, esi_checkpoint_path, device):\n    \"\"\"\n    Load trained model from checkpoint\n    \n    Args:\n        checkpoint_path: Path to best_model.pt\n        esi_checkpoint_path: Path to ESI model.safetensors\n        device: torch device\n    \n    Returns:\n        model, config, normalization_params\n    \"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"üì• LOADING TRAINED MODEL\")\n    print(\"=\"*70)\n    \n    try:\n        # Load checkpoint with weights_only=False for PyTorch 2.6+\n        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n        \n        # Extract configuration\n        model_config = checkpoint.get('config', {})\n        normalization = checkpoint.get('normalization', {'mean': 0.0, 'std': 1.0})\n        \n        print(f\"‚úÖ Checkpoint loaded from epoch {checkpoint.get('epoch', 'unknown')}\")\n        print(f\"‚úÖ Validation loss: {checkpoint.get('val_loss', 'N/A')}\")\n        \n        # Create config object\n        class InferenceConfig:\n            SEQ_LENGTH = model_config.get('seq_length', 6000)\n            ESI_EMBEDDING_DIM = model_config.get('esi_embedding_dim', 1024)\n            HIDDEN_DIM = model_config.get('hidden_dim', 256)\n            LSTM_LAYERS = model_config.get('lstm_layers', 2)\n            DROPOUT = model_config.get('dropout', 0.3)\n            DEVICE = device\n        \n        config = InferenceConfig()\n        \n        # Initialize model\n        print(\"\\nüèóÔ∏è Initializing model architecture...\")\n        model = ESIApneaDetector(esi_checkpoint_path, config)\n        \n        # Load trained weights\n        model.load_state_dict(checkpoint['model_state_dict'])\n        model.to(device)\n        model.eval()\n        \n        print(f\"\\nüìä Model Configuration:\")\n        print(f\"   Sequence Length:    {config.SEQ_LENGTH}\")\n        print(f\"   ESI Embedding Dim:  {config.ESI_EMBEDDING_DIM}\")\n        print(f\"   Hidden Dimension:   {config.HIDDEN_DIM}\")\n        print(f\"   LSTM Layers:        {config.LSTM_LAYERS}\")\n        print(f\"\\nüî¢ Normalization Parameters:\")\n        print(f\"   Mean: {normalization['mean']:.6f}\")\n        print(f\"   Std:  {normalization['std']:.6f}\")\n        print(\"=\"*70)\n        \n        return model, config, normalization\n    \n    except Exception as e:\n        print(f\"‚ùå Error loading model: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None, None, None\n\n\n# ==============================================================================\n# Load Your Trained Model\n# ==============================================================================\n\n# Define paths\nBEST_MODEL_PATH = \"/kaggle/input/model-checkpoints-apnea/outputs/checkpoints/best_model.pt\"\nESI_CHECKPOINT_PATH = \"/kaggle/input/esi-model/model.safetensors\"\n\n# Load model\nmodel, config, normalization_params = load_trained_model(\n    BEST_MODEL_PATH,\n    ESI_CHECKPOINT_PATH,\n    device\n)\n\nif model is None:\n    raise RuntimeError(\"Failed to load model!\")\n\nprint(\"\\n‚úÖ Model loaded successfully and ready for inference!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T16:09:42.528609Z","iopub.execute_input":"2025-12-14T16:09:42.531184Z","iopub.status.idle":"2025-12-14T16:09:52.581815Z","shell.execute_reply.started":"2025-12-14T16:09:42.531138Z","shell.execute_reply":"2025-12-14T16:09:52.580703Z"}},"outputs":[{"name":"stdout","text":"üñ•Ô∏è Using device: cpu\n\n======================================================================\nüì• LOADING TRAINED MODEL\n======================================================================\n‚úÖ Checkpoint loaded from epoch 3\n‚úÖ Validation loss: 0.41083619382345316\n\nüèóÔ∏è Initializing model architecture...\n‚úÖ ESI weights loaded\n\nüìä Model Configuration:\n   Sequence Length:    6000\n   ESI Embedding Dim:  1024\n   Hidden Dimension:   256\n   LSTM Layers:        2\n\nüî¢ Normalization Parameters:\n   Mean: 0.000000\n   Std:  1.000000\n======================================================================\n\n‚úÖ Model loaded successfully and ready for inference!\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# ==============================================================================\n# CELL 2: Flask API with Ngrok Deployment\n# ==============================================================================\n\nimport os\nimport sys\n\n# Install dependencies\nprint(\"üì¶ Installing dependencies...\")\nos.system(\"killall ngrok 2>/dev/null\")\nos.system('pip install -q flask flask-cors pyngrok')\n\nfrom flask import Flask, request, jsonify\nfrom flask_cors import CORS\nfrom pyngrok import ngrok, conf\nfrom kaggle_secrets import UserSecretsClient\nimport numpy as np\nimport torch\nimport wfdb\nfrom werkzeug.utils import secure_filename\nfrom datetime import datetime\nimport traceback\n\n# ==============================================================================\n# Ngrok Authentication\n# ==============================================================================\n\nprint(\"\\nüîê Setting up Ngrok...\")\ntry:\n    user_secrets = UserSecretsClient()\n    NGROK_AUTH_TOKEN = user_secrets.get_secret(\"NGROK_AUTH_TOKEN\")\n    \n    # Optional: Use static domain if you have one\n    try:\n        NGROK_STATIC_DOMAIN = user_secrets.get_secret(\"NGROK_STATIC_DOMAIN\")\n        USE_STATIC_DOMAIN = True\n        print(f\"‚úÖ Static domain found: {NGROK_STATIC_DOMAIN}\")\n    except:\n        USE_STATIC_DOMAIN = False\n        print(\"‚ÑπÔ∏è No static domain configured (using dynamic URL)\")\n    \n    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n    conf.get_default().region = \"us\"\n    print(\"‚úÖ Ngrok authenticated\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Ngrok setup failed: {e}\")\n    print(\"\\nüìù To fix this:\")\n    print(\"   1. Go to https://dashboard.ngrok.com/\")\n    print(\"   2. Copy your authtoken\")\n    print(\"   3. Add it as a Kaggle Secret:\")\n    print(\"      - Key: NGROK_AUTH_TOKEN\")\n    print(\"      - Value: <your-token>\")\n    raise\n\n# ==============================================================================\n# Flask App Setup\n# ==============================================================================\n\napp = Flask(__name__)\nCORS(app)\n\nUPLOAD_FOLDER = '/tmp/ecg_uploads'\nALLOWED_EXTENSIONS = {'hea', 'dat'}\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\n\n# ==============================================================================\n# Helper Functions\n# ==============================================================================\n\ndef allowed_file(filename):\n    \"\"\"Check if file extension is allowed\"\"\"\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\n\ndef extract_demographics(record_path):\n    \"\"\"Extract patient demographics from WFDB header\"\"\"\n    try:\n        header = wfdb.rdheader(record_path)\n        demographics = {\n            'record_name': header.record_name,\n            'sampling_frequency': f\"{header.fs} Hz\",\n            'signal_length': f\"{header.sig_len} samples\",\n            'duration': f\"{header.sig_len / header.fs:.1f} seconds\",\n            'number_of_signals': header.n_sig,\n        }\n        \n        # Extract additional info from comments\n        if hasattr(header, 'comments') and header.comments:\n            for comment in header.comments:\n                comment_lower = comment.lower()\n                if 'age' in comment_lower:\n                    demographics['age'] = comment\n                elif 'sex' in comment_lower or 'gender' in comment_lower:\n                    demographics['gender'] = comment\n        \n        return demographics\n    except Exception as e:\n        return {'error': str(e)}\n\n\ndef generate_waveform_points(signal, target_points=800):\n    \"\"\"Generate SVG polyline points for waveform visualization\"\"\"\n    try:\n        # Downsample signal for visualization\n        downsample_factor = max(1, len(signal) // target_points)\n        signal_viz = signal[::downsample_factor]\n        \n        # Normalize to 0-100 range (inverted for SVG)\n        signal_min, signal_max = signal_viz.min(), signal_viz.max()\n        \n        if abs(signal_max - signal_min) < 1e-6:\n            signal_normalized = np.full_like(signal_viz, 50.0)\n        else:\n            signal_normalized = 100 - ((signal_viz - signal_min) / (signal_max - signal_min) * 80 + 10)\n        \n        # Create SVG points\n        x_scale = 800 / len(signal_normalized)\n        points = [f\"{i * x_scale:.2f},{y:.2f}\" for i, y in enumerate(signal_normalized)]\n        \n        return \" \".join(points)\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Waveform generation error: {e}\")\n        return \"0,50 800,50\"\n\n\ndef preprocess_signal(signal, mean, std, seq_length=6000):\n    \"\"\"\n    Preprocess ECG signal for model inference\n    \n    Args:\n        signal: Raw ECG signal (numpy array)\n        mean: Normalization mean\n        std: Normalization std\n        seq_length: Target sequence length\n    \n    Returns:\n        Preprocessed signal tensor [1, 1, seq_length]\n    \"\"\"\n    try:\n        # Handle multi-channel signals (take first channel)\n        if signal.ndim > 1:\n            signal = signal[:, 0]\n        \n        # Pad or truncate to fixed length\n        if len(signal) < seq_length:\n            signal = np.pad(signal, (0, seq_length - len(signal)), mode='edge')\n        elif len(signal) > seq_length:\n            signal = signal[:seq_length]\n        \n        # Normalize using provided parameters\n        signal_normalized = (signal - mean) / (std + 1e-8)\n        \n        # Convert to tensor with shape [batch, channels, length]\n        signal_tensor = torch.tensor(signal_normalized, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n        \n        return signal_tensor\n    \n    except Exception as e:\n        print(f\"‚ùå Preprocessing error: {e}\")\n        return None\n\n\ndef run_inference(model, signal_tensor, device):\n    \"\"\"\n    Run model inference and return prediction\n    \n    Args:\n        model: Trained model\n        signal_tensor: Preprocessed signal tensor\n        device: torch device\n    \n    Returns:\n        Dictionary with prediction results\n    \"\"\"\n    try:\n        model.eval()\n        \n        with torch.no_grad():\n            output = model(signal_tensor.to(device))\n            raw_logit = output.cpu().item()\n            probability = torch.sigmoid(output).cpu().item()\n        \n        has_apnea = probability > 0.5\n        \n        # Calculate risk level\n        if probability > 0.7:\n            risk_level = \"High Risk\"\n            risk_color = \"danger\"\n        elif probability > 0.4:\n            risk_level = \"Moderate Risk\"\n            risk_color = \"warning\"\n        else:\n            risk_level = \"Low Risk\"\n            risk_color = \"success\"\n        \n        result = {\n            'has_apnea': bool(has_apnea),\n            'probability': float(probability),\n            'raw_logit': float(raw_logit),\n            'risk_level': risk_level,\n            'risk_color': risk_color,\n            'diagnosis': 'Apnea Detected' if has_apnea else 'Normal Breathing'\n        }\n        \n        print(f\"\\n{'='*50}\")\n        print(f\"üîç INFERENCE RESULT:\")\n        print(f\"{'='*50}\")\n        print(f\"Diagnosis:    {result['diagnosis']}\")\n        print(f\"Probability:  {probability:.4f}\")\n        print(f\"Risk Level:   {risk_level}\")\n        print(f\"{'='*50}\\n\")\n        \n        return result\n    \n    except Exception as e:\n        print(f\"‚ùå Inference error: {e}\")\n        traceback.print_exc()\n        return None\n\n\n# ==============================================================================\n# API Endpoints\n# ==============================================================================\n\n@app.route('/health', methods=['GET'])\ndef health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return jsonify({\n        'status': 'healthy',\n        'model_loaded': model is not None,\n        'device': str(device),\n        'gpu_available': torch.cuda.is_available(),\n        'normalization': normalization_params,\n        'timestamp': datetime.utcnow().isoformat()\n    })\n\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    \"\"\"\n    Main prediction endpoint\n    \n    Expected: POST request with files containing .hea and .dat files\n    Returns: JSON with prediction results, demographics, and waveform data\n    \"\"\"\n    try:\n        # Validate model is loaded\n        if model is None:\n            return jsonify({\n                'success': False,\n                'error': 'Model not loaded'\n            }), 500\n        \n        # Validate files in request\n        if 'files' not in request.files:\n            return jsonify({\n                'success': False,\n                'error': 'No files uploaded'\n            }), 400\n        \n        files = request.files.getlist('files')\n        \n        # Find .hea and .dat files\n        hea_file = next((f for f in files if f.filename.endswith('.hea')), None)\n        dat_file = next((f for f in files if f.filename.endswith('.dat')), None)\n        \n        if not hea_file or not dat_file:\n            return jsonify({\n                'success': False,\n                'error': 'Both .hea and .dat files required'\n            }), 400\n        \n        # Save uploaded files\n        base_name = secure_filename(hea_file.filename).rsplit('.', 1)[0]\n        hea_path = os.path.join(UPLOAD_FOLDER, f\"{base_name}.hea\")\n        dat_path = os.path.join(UPLOAD_FOLDER, f\"{base_name}.dat\")\n        \n        hea_file.save(hea_path)\n        dat_file.save(dat_path)\n        \n        print(f\"\\nüìÅ Processing record: {base_name}\")\n        \n        # Load ECG record\n        record_path = os.path.join(UPLOAD_FOLDER, base_name)\n        demographics = extract_demographics(record_path)\n        \n        # Read signal (first 6000 samples or configured length)\n        signal, _ = wfdb.rdsamp(record_path, sampfrom=0, sampto=config.SEQ_LENGTH)\n        \n        # Generate waveform visualization\n        raw_signal = signal[:, 0] if signal.ndim > 1 else signal\n        waveform_points = generate_waveform_points(raw_signal)\n        \n        # Preprocess signal\n        signal_tensor = preprocess_signal(\n            signal,\n            normalization_params['mean'],\n            normalization_params['std'],\n            config.SEQ_LENGTH\n        )\n        \n        if signal_tensor is None:\n            return jsonify({\n                'success': False,\n                'error': 'Signal preprocessing failed'\n            }), 500\n        \n        # Run inference\n        prediction_result = run_inference(model, signal_tensor, device)\n        \n        if prediction_result is None:\n            return jsonify({\n                'success': False,\n                'error': 'Model inference failed'\n            }), 500\n        \n        # Build response\n        response = {\n            'success': True,\n            'prediction': prediction_result,\n            'demographics': demographics,\n            'waveform': waveform_points,\n            'timestamp': datetime.utcnow().isoformat()\n        }\n        \n        # Cleanup uploaded files\n        try:\n            os.remove(hea_path)\n            os.remove(dat_path)\n        except:\n            pass\n        \n        print(f\"‚úÖ Analysis complete: {prediction_result['diagnosis']}\")\n        \n        return jsonify(response), 200\n    \n    except Exception as e:\n        print(f\"\\n‚ùå ERROR in /predict:\")\n        traceback.print_exc()\n        return jsonify({\n            'success': False,\n            'error': str(e)\n        }), 500\n\n\n@app.route('/', methods=['GET'])\ndef home():\n    \"\"\"Root endpoint with API information\"\"\"\n    return jsonify({\n        'message': 'Sleep Apnea Detection API',\n        'version': '1.0',\n        'endpoints': {\n            '/health': 'GET - Check API health',\n            '/predict': 'POST - Predict sleep apnea from ECG files'\n        },\n        'usage': 'Send .hea and .dat files to /predict endpoint'\n    })\n\n\n# ==============================================================================\n# Start Server\n# ==============================================================================\n\ndef start_server():\n    \"\"\"Start Flask server with Ngrok tunnel\"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"üöÄ STARTING INFERENCE SERVER\")\n    print(\"=\"*70)\n    \n    if model is None:\n        print(\"‚ùå ERROR: Model not loaded!\")\n        return\n    \n    try:\n        port = 5000\n        \n        # Create ngrok tunnel\n        if USE_STATIC_DOMAIN:\n            # Use static domain (requires paid ngrok plan)\n            public_url = ngrok.connect(\n                port,\n                bind_tls=True,\n                hostname=NGROK_STATIC_DOMAIN\n            )\n            display_url = f\"https://{NGROK_STATIC_DOMAIN}\"\n        else:\n            # Use dynamic URL (free tier)\n            public_url = ngrok.connect(port, bind_tls=True)\n            display_url = public_url.public_url\n        \n        print(\"\\n\" + \"=\"*70)\n        print(\"‚úÖ SERVER READY\")\n        print(\"=\"*70)\n        print(f\"üåç Public URL:       {display_url}\")\n        print(f\"üîç Health Check:     {display_url}/health\")\n        print(f\"üì° Predict Endpoint: {display_url}/predict\")\n        print(\"=\"*70)\n        print(\"\\nüìù Example cURL command:\")\n        print(f\"\"\"\ncurl -X POST {display_url}/predict \\\\\n  -F \"files=@path/to/record.hea\" \\\\\n  -F \"files=@path/to/record.dat\"\n        \"\"\")\n        print(\"=\"*70 + \"\\n\")\n        \n        # Start Flask app\n        app.run(host='0.0.0.0', port=port, debug=False, use_reloader=False)\n    \n    except Exception as e:\n        print(f\"\\n‚ùå Server start failed: {e}\")\n        traceback.print_exc()\n\n\n# ==============================================================================\n# Auto-start if model is loaded\n# ==============================================================================\n\nif 'model' in globals() and model is not None:\n    print(\"\\n‚úÖ Model detected - starting server...\")\n    start_server()\nelse:\n    print(\"\\n‚ö†Ô∏è Model not found. Please run the model loading cell first.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T16:10:04.868898Z","iopub.execute_input":"2025-12-14T16:10:04.869241Z"}},"outputs":[{"name":"stdout","text":"üì¶ Installing dependencies...\n\nüîê Setting up Ngrok...\n‚úÖ Static domain found: merry-ewe-endlessly.ngrok-free.app\n‚úÖ Ngrok authenticated\n\n‚úÖ Model detected - starting server...\n\n======================================================================\nüöÄ STARTING INFERENCE SERVER\n======================================================================\n\n======================================================================\n‚úÖ SERVER READY\n======================================================================\nüåç Public URL:       https://merry-ewe-endlessly.ngrok-free.app\nüîç Health Check:     https://merry-ewe-endlessly.ngrok-free.app/health\nüì° Predict Endpoint: https://merry-ewe-endlessly.ngrok-free.app/predict\n======================================================================\n\nüìù Example cURL command:\n\ncurl -X POST https://merry-ewe-endlessly.ngrok-free.app/predict \\\n  -F \"files=@path/to/record.hea\" \\\n  -F \"files=@path/to/record.dat\"\n        \n======================================================================\n\n * Serving Flask app '__main__'\n * Debug mode: off\n","output_type":"stream"},{"name":"stderr","text":"WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n * Running on all addresses (0.0.0.0)\n * Running on http://127.0.0.1:5000\n * Running on http://172.19.2.2:5000\nPress CTRL+C to quit\n","output_type":"stream"},{"name":"stdout","text":"\nüìÅ Processing record: a01\n","output_type":"stream"},{"name":"stderr","text":"127.0.0.1 - - [14/Dec/2025 16:10:35] \"POST /predict HTTP/1.1\" 200 -\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nüîç INFERENCE RESULT:\n==================================================\nDiagnosis:    Apnea Detected\nProbability:  0.5895\nRisk Level:   Moderate Risk\n==================================================\n\n‚úÖ Analysis complete: Apnea Detected\n\nüìÅ Processing record: a03\n","output_type":"stream"},{"name":"stderr","text":"127.0.0.1 - - [14/Dec/2025 16:10:51] \"POST /predict HTTP/1.1\" 200 -\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nüîç INFERENCE RESULT:\n==================================================\nDiagnosis:    Normal Breathing\nProbability:  0.1381\nRisk Level:   Low Risk\n==================================================\n\n‚úÖ Analysis complete: Normal Breathing\n\nüìÅ Processing record: a07\n","output_type":"stream"},{"name":"stderr","text":"127.0.0.1 - - [14/Dec/2025 16:11:12] \"POST /predict HTTP/1.1\" 200 -\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nüîç INFERENCE RESULT:\n==================================================\nDiagnosis:    Apnea Detected\nProbability:  0.6918\nRisk Level:   Moderate Risk\n==================================================\n\n‚úÖ Analysis complete: Apnea Detected\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}